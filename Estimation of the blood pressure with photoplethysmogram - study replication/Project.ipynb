{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U PPGBP datasetu nemamo kolone sa null vrednostima osim ['ppg_fft_peaks_0', 'ppg_fft_peaks_heights_0', 'ppg_fft_peaks_neighbor_avgs_0'] koje imaju sve null vrednosti. U ovom setu podataka imamo oznaku za pacijenta i za merenje i mislim da su sa SP i DP oznaceni gornji i donji krvni pritisak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    '''Read folds for feature to label training and features importances'''\n",
    "    filtered_paths = [i for i in os.listdir(file_name) if i.startswith('feat')]\n",
    "    loaded_files = [pd.read_csv(os.path.join(file_name,path)) for path in filtered_paths]\n",
    "\n",
    "    if file_name == 'ppgbp_dataset':\n",
    "        for file in loaded_files:\n",
    "            file.dropna(axis=1,inplace=True)\n",
    "    if file_name == 'uci2_dataset':\n",
    "        for file in loaded_files:\n",
    "            file.dropna(axis=0,inplace=True)\n",
    "        \n",
    "\n",
    "    dp_string = f\"bp-benchmark-main\\\\feat_importance\\\\{file_name}\\\\featImportance-DP.pkl\"\n",
    "    sp_string = f\"bp-benchmark-main\\\\feat_importance\\\\{file_name}\\\\featImportance-SP.pkl\"\n",
    "\n",
    "    dp_feature_importance = pickle.load(open(dp_string,'rb'))\n",
    "    sp_feature_importance = pickle.load(open(sp_string,'rb'))\n",
    "\n",
    "    return loaded_files, [dp_feature_importance,sp_feature_importance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_folds(loaded_files):\n",
    "    '''Takes in different df folds and returns indexes that represent these exact folds in GridSearchCV function'''\n",
    "    num_folds = len(loaded_files)\n",
    "    lower_bound = 0\n",
    "    upper_bound = 0\n",
    "    indices = []\n",
    "    for fold in range(num_folds):\n",
    "        upper_bound += len(loaded_files[fold])\n",
    "        indices.append(list(range(lower_bound,upper_bound)))\n",
    "        lower_bound = upper_bound\n",
    "\n",
    "    CVs =[]\n",
    "    for i in range(num_folds):\n",
    "        test_indices = indices[i]\n",
    "        train_indices = [indices[j] for j in range(num_folds) if j !=i]\n",
    "        train_indices = np.concatenate(train_indices)\n",
    "\n",
    "        CVs.append((train_indices,test_indices))\n",
    "    return CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_models(num_features=7):\n",
    "    '''Trains all models for sytolic and dystoli pressure and returns results. Training is done using GridSearch to \n",
    "    find optimal parameters and with different number of features.'''\n",
    "\n",
    "    def load_grid_searches():\n",
    "        \"\"\"Initializes GridSearch object that consists of ML models and appropriate parameters\"\"\"\n",
    "        grid_searches = []\n",
    "        for model, param_grid in zip(models,param_grids):\n",
    "            grid_search = GridSearchCV(model,param_grid,cv =CVs,scoring='neg_mean_absolute_error',n_jobs=-1)\n",
    "            grid_searches.append(grid_search)\n",
    "        return grid_searches\n",
    "    \n",
    "\n",
    "    def train_grid_searches(grid_searches,X,y,num_feature): \n",
    "        \"\"\"Trains grid searches defines in function 'load_grid_searches', displays and returns results.\"\"\"\n",
    "        results = [] \n",
    "        for grid_search in grid_searches:\n",
    "\n",
    "            print(f'Traning {grid_search.estimator} model')\n",
    "            grid_search.fit(X,y)\n",
    "            y_pred = grid_search.best_estimator_.predict(X)\n",
    "            error = y_pred-y\n",
    "            \n",
    "            print('\\n')\n",
    "\n",
    "            #grid_score = -grid_search.score(X,y)\n",
    "            grid_score = grid_search.best_score_\n",
    "            print('Overall score', grid_score)\n",
    "            print(f'ME +- SD:{np.mean(error)} +- {np.std(error)}')\n",
    "            naive_error = mean_absolute_error(y,np.full(y.shape,np.mean(y)))\n",
    "            mase = grid_score/naive_error\n",
    "            print('MASE score: ',mase)\n",
    "\n",
    "            # Showing results of cross validation\n",
    "            columns_of_interest = ['params','mean_test_score','std_test_score'] + [f'split{i}_test_score' for i in range(5)]\n",
    "            display(pd.DataFrame({col: grid_search.cv_results_[col] for col in columns_of_interest}))\n",
    "\n",
    "            print('Best params:',grid_search.best_params_)\n",
    "            print('\\n\\n')\n",
    "            results.append([grid_search.estimator,grid_search.best_params_,num_feature,grid_score,np.mean(error),np.std(error),mase])\n",
    "        return pd.DataFrame(results,columns = ['Model','Parameters','Number of features','Overall score','ME','SD','MASE'])\n",
    "\n",
    "    def train(type = 'DP'):    \n",
    "        \"\"\"Performs training with different number of features either for dystolic of systolic pressure\"\"\"        \n",
    "        feature_row = 0 if 'DP' else 1\n",
    "        if isinstance(num_features,list):\n",
    "            results = []\n",
    "            for num_feature in num_features:\n",
    "                print('Number of features is:',num_feature,'\\n')\n",
    "                features = feature_importances[feature_row].features[:num_feature]\n",
    "                X = data[features]\n",
    "                y = data[type]\n",
    "\n",
    "                grid_searches = load_grid_searches()\n",
    "                result = train_grid_searches(grid_searches,X,y,num_feature)\n",
    "                results.append(result)\n",
    "            return pd.concat(results)\n",
    "        else: \n",
    "            features = feature_importances[feature_row].features[:num_features]\n",
    "            X = data[features]\n",
    "            y = data[type]\n",
    "\n",
    "            grid_searches = load_grid_searches()\n",
    "            print(f'Training {type}')\n",
    "            train_grid_searches(grid_searches)\n",
    "\n",
    "    print('Training DP')\n",
    "    dp_results = train(type='DP')\n",
    "    \n",
    "    print('Training SP')\n",
    "    sp_results = train(type='SP')\n",
    "    \n",
    "    return [dp_results,sp_results]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [SVR(),\n",
    "          MLPRegressor(),\n",
    "          RandomForestRegressor(),\n",
    "          ]\n",
    "\n",
    "svr_param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],  \n",
    "    'C': [0.1, 1, 10, 100],                                      \n",
    "    'epsilon': [0.1, 0.2, 0.3]           \n",
    "}\n",
    "\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(25,), (50,), (25, 25), (50, 25)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.01,0.1],\n",
    "    'learning_rate': ['adaptive'],\n",
    "    'max_iter': [400]\n",
    "}\n",
    "\n",
    "\n",
    "forest_param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "\n",
    "param_grids = [svr_param_grid, mlp_param_grid, forest_param_grid]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [4,8,12,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_files, feature_importances = read_data('ppgbp_dataset')\n",
    "CVs = custom_folds(loaded_files)\n",
    "data = pd.concat(loaded_files,ignore_index=True)\n",
    "\n",
    "results_dp_ppgbp,results_sp_ppgbp = fit_models(num_features = num_features)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenutak = time.strftime('%H_%M')\n",
    "results_dp_ppgbp.to_csv(f'results_dp_ppgbp_{trenutak}.csv')\n",
    "results_sp_ppgbp.to_csv(f'results_sp_ppgbp_{trenutak}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_files, feature_importances = read_data('sensors_dataset')\n",
    "CVs = custom_folds(loaded_files)\n",
    "data = pd.concat(loaded_files,ignore_index=True)\n",
    "\n",
    "results_dp_sensor, results_sp_sensor = fit_models(num_features = num_features)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenutak = time.strftime('%H_%M')\n",
    "results_dp_sensor.to_csv(f'results_dp_sensor_{trenutak}.csv')\n",
    "results_sp_sensor.to_csv(f'results_sp_sensor_{trenutak}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_files, feature_importances = read_data('bcg_dataset')\n",
    "CVs = custom_folds(loaded_files)\n",
    "data = pd.concat(loaded_files,ignore_index=True)\n",
    "\n",
    "results_dp_bcg,results_sp_bcg = fit_models(num_features = num_features)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trenutak = time.strftime('%H_%M')\n",
    "results_dp_bcg.to_csv(f'results_dp_bcg_{trenutak}.csv')\n",
    "results_sp_bcg.to_csv(f'results_sp_bcg_{trenutak}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dp_bcg.sort_values('Overall score')\n",
    "results_dp_bcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sp_bcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
